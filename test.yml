volumes:
  open-webui:   
  ollama_storage: 

networks:
  demo:         

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    networks: ['demo']
    restart: unless-stopped
    ports:
      - 11435:11434
    volumes:
      - ollama_storage:/root/.ollama
    command: >
      /bin/sh -c "sleep 3; OLLAMA_HOST=ollama:11434 ollama pull llama3.2:1b; OLLAMA_HOST=ollama:11434 ollama start llama3.2:1b"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    networks: ['demo']
    restart: unless-stopped
    container_name: open-webui_container
    ports:
      - "3000:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - open-webui:/app/backend/data  
